{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Image Segmenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gxbHBsF-8Y_l"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEPRECATION: wfuzz 3.1.0 has a non-standard dependency specifier pyparsing>=2.4*; python_version >= \"3.5\". pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of wfuzz or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
          ]
        }
      ],
      "source": [
        "!pip install -q mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "## Download the image segmenter model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHXsuWDxpOj4"
      },
      "source": [
        "The next thing you will need to do is download the image segmentation model that will be used for this demo. In this case we will use the DeepLab v3 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://storage.googleapis.com/mediapipe-models/image_segmenter/deeplab_v3/float32/1/deeplab_v3.tflite\"\n",
        "file_path = \"deeplabv3.tflite\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(file_path, \"wb\") as file:\n",
        "    file.write(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## Add a test image\n",
        "\n",
        "After downloading the model, it's time to grab an image that you can use for testing! It's worth noting that while this is working with a single image, you can download a collection of images to store in the `IMAGE_FILENAMES` array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "tzXuqyIBlXer"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "\n",
        "IMAGE_FILENAMES = ['bilal2.jpg'] #INSERT JPG FILE PATH HERE. ONE IMAGE PATH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8XRmapjySMN"
      },
      "source": [
        "## Preview the downloaded image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu_q-Z03r-ed"
      },
      "source": [
        "With the test image downloaded, go ahead and display it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8rjHk72-lmHX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bilal2.jpg\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import math\n",
        "\n",
        "# Height and width that will be used by the model\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "\n",
        "# Performs resizing and showing the image\n",
        "def resize_and_show(image):\n",
        "    h, w = image.shape[:2]\n",
        "    if h < w:\n",
        "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h / (w / DESIRED_WIDTH))))\n",
        "    else:\n",
        "        img = cv2.resize(image, (math.floor(w / (h / DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "    cv2.imshow('Image', img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Preview the image(s)\n",
        "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
        "for name, image in images.items():\n",
        "    print(name)\n",
        "    resize_and_show(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Yl_Oiye4mUuo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bilal2.jpg']\n",
            "Saved silhouette image as: bilal2_silhouette.jpg\n",
            "Segmentation mask of bilal2.jpg:\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "\n",
        "BG_COLOR = (0, 0, 0) # black\n",
        "MASK_COLOR = (255, 255, 255) # white\n",
        "\n",
        "\n",
        "# Create the options that will be used for ImageSegmenter\n",
        "base_options = python.BaseOptions(model_asset_path='deeplabv3.tflite')\n",
        "options = vision.ImageSegmenterOptions(base_options=base_options,\n",
        "                                       output_category_mask=True)\n",
        "\n",
        "print(IMAGE_FILENAMES)\n",
        "# Create the image segmenter\n",
        "with vision.ImageSegmenter.create_from_options(options) as segmenter:\n",
        "\n",
        "  # Loop through demo image(s)\n",
        "  for image_file_name in IMAGE_FILENAMES:\n",
        "\n",
        "    # Create the MediaPipe image file that will be segmented\n",
        "    image = mp.Image.create_from_file(image_file_name)\n",
        "\n",
        "    # Retrieve the masks for the segmented image\n",
        "    segmentation_result = segmenter.segment(image)\n",
        "    category_mask = segmentation_result.category_mask\n",
        "\n",
        "    # Generate solid color images for showing the output segmentation mask.\n",
        "    image_data = image.numpy_view()\n",
        "    fg_image = np.zeros(image_data.shape, dtype=np.uint8)\n",
        "    fg_image[:] = MASK_COLOR\n",
        "    bg_image = np.zeros(image_data.shape, dtype=np.uint8)\n",
        "    bg_image[:] = BG_COLOR\n",
        "\n",
        "    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.2\n",
        "    output_image = np.where(condition, fg_image, bg_image)\n",
        "\n",
        "    output_file_name = f\"{image_file_name.split('.')[0]}_silhouette.jpg\"\n",
        "    cv2.imwrite(output_file_name, output_image)\n",
        "\n",
        "    print(f\"Saved silhouette image as: {output_file_name}\")\n",
        "\n",
        "    print(f'Segmentation mask of {name}:')\n",
        "    resize_and_show(output_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"final_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    # Resize image to (64, 64)\n",
        "    resized_image = cv2.resize(image, (128, 128))\n",
        "    \n",
        "    # Convert to RGBA format\n",
        "    rgba_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGBA)\n",
        "\n",
        "    # Convert to float32 and normalize\n",
        "    normalized_image = rgba_image.astype(np.float32) / 255.0\n",
        "\n",
        "    # Add batch dimension\n",
        "    preprocessed_image = np.expand_dims(normalized_image, axis=0)\n",
        "\n",
        "    return preprocessed_image\n",
        "\n",
        "def postprocess_image(image):\n",
        "    return image*256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_image = preprocess_image(output_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        }
      ],
      "source": [
        "data = postprocess_image(model.predict(final_image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The predicted body measurements in cm are:\n",
            "+---------+--------------+---------+--------+---------+-----------+----------+---------+--------------+--------------------+----------------------+---------+---------+---------+\n",
            "|   ankle |   arm-length |   bicep |   calf |   chest |   forearm |   height |     hip |   leg-length |   shoulder-breadth |   shoulder-to-crotch |   thigh |   waist |   wrist |\n",
            "+=========+==============+=========+========+=========+===========+==========+=========+==============+====================+======================+=========+=========+=========+\n",
            "| 22.0463 |      35.4276 | 26.4882 | 31.174 | 86.3326 |   20.5838 |  144.919 | 91.5133 |      63.7362 |            31.2414 |              59.4454 | 49.3582 | 78.7543 |  11.074 |\n",
            "+---------+--------------+---------+--------+---------+-----------+----------+---------+--------------+--------------------+----------------------+---------+---------+---------+\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "# Headers in the same order as the data\n",
        "headers = [\"ankle\", \"arm-length\", \"bicep\", \"calf\", \"chest\", \"forearm\", \"height\",\n",
        "           \"hip\", \"leg-length\", \"shoulder-breadth\", \"shoulder-to-crotch\", \"thigh\", \"waist\", \"wrist\"]\n",
        "\n",
        "\n",
        "# Present the data in tabular format\n",
        "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "print(\"The predicted body measurements in cm are:\")\n",
        "print(table)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
